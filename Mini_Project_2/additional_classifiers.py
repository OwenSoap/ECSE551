# -*- coding: utf-8 -*-
"""Additional Classifiers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jQsRTZihNlPC99pBzRYqvTbowAY-tqi1

<center><h1>Mini Project 2 - Bernoulli Naïve Bayes</h1>
<h4>This file records the accuracies of the combinations of 8 classifiers and 5 vectorizers.</h4></center>

<h3>Team Members:</h3>
<center>
Yi Zhu, 260716006<br>
Fei Peng, 260712440<br>
Yukai Zhang, 260710915
</center>
"""

from google.colab import drive
drive.mount('/content/drive')

# make path = './' in-case you are running this locally
path = '/content/drive/My Drive/ECSE_551_F_2020/Mini_Project_02/'

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import Normalizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.feature_extraction import text
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, cross_val_score, KFold
from sklearn.pipeline import make_pipeline

!pip install nltk
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

from nltk.stem import PorterStemmer
from nltk import word_tokenize
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

"""Additional classifiers:  
1. Logistic Regression
2. Multinomial Naïve Bayes
3. Support Vector Machine
4. Random Forest
5. Decision Tree
6. Ada Boost
7. k-Neighbors
8. Neural Network
"""

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier

reddit_dataset = pd.read_csv(path+"train.csv")
reddit_test = pd.read_csv(path+"test.csv")

X = reddit_dataset['body']
y = reddit_dataset['subreddit']

"""# Define Vectorizer
### (To vectorize the text-based data to numerical features)

1. CountVectorizer  
1) Use "CountVectorizer" to transform text data to feature vectors.  
2) Normalize your feature vectors
"""

def count_vectorizer(X_train, X_test):
    vectorizer = CountVectorizer()
    vectors_train = vectorizer.fit_transform(X_train)
    vectors_test = vectorizer.transform(X_test)

    normalizer_train = Normalizer().fit(X=vectors_train)
    vectors_train = normalizer_train.transform(vectors_train)
    vectors_test = normalizer_train.transform(vectors_test)

    return vectors_train, vectors_test

"""2. CountVectorizer with stop word  
1) Use "CountVectorizer" with stop word to transform text data to vector.  
2) Normalize your feature vectors
"""

def count_vec_with_sw(X_train, X_test):
    stop_words = text.ENGLISH_STOP_WORDS
    vectorizer = CountVectorizer(stop_words=stop_words)
    vectors_train_stop = vectorizer.fit_transform(X_train)
    vectors_test_stop = vectorizer.transform(X_test)

    normalizer_train = Normalizer().fit(X=vectors_train_stop)
    vectors_train_stop= normalizer_train.transform(vectors_train_stop)
    vectors_test_stop = normalizer_train.transform(vectors_test_stop)

    return vectors_train_stop, vectors_test_stop

"""3. TF-IDF  
1) use "TfidfVectorizer" to weight features based on your train set.  
2) Normalize your feature vectors
"""

def tfidf_vectorizer(X_train, X_test):
    tf_idf_vectorizer = TfidfVectorizer()
    vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)
    vectors_test_idf = tf_idf_vectorizer.transform(X_test)

    normalizer_train = Normalizer().fit(X=vectors_train_idf)
    vectors_train_idf= normalizer_train.transform(vectors_train_idf)
    vectors_test_idf = normalizer_train.transform(vectors_test_idf)

    return vectors_train_idf, vectors_test_idf

"""4. CountVectorizer with stem tokenizer  
1) Use "StemTokenizer" to transform text data to vector.  
2) Normalize your feature vectors
"""

class StemTokenizer:
     def __init__(self):
       self.wnl =PorterStemmer()
     def __call__(self, doc):
       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]


def count_vec_stem(X_train, X_test):
    vectorizer = CountVectorizer(tokenizer=StemTokenizer())
    vectors_train_stem = vectorizer.fit_transform(X_train)
    vectors_test_stem = vectorizer.transform(X_test)

    normalizer_train = Normalizer().fit(X=vectors_train_stem)
    vectors_train_stem= normalizer_train.transform(vectors_train_stem)
    vectors_test_stem = normalizer_train.transform(vectors_test_stem)

    return vectors_train_stem, vectors_test_stem

"""5. CountVectorizer with lemma tokenizer  
1) Use "LemmaTokenizer" to transform text data to vector.  
2) Normalize your feature vectors
"""

def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)


class LemmaTokenizer:
     def __init__(self):
       self.wnl = WordNetLemmatizer()
     def __call__(self, doc):
       return [self.wnl.lemmatize(t,pos =get_wordnet_pos(t)) for t in word_tokenize(doc) if t.isalpha()]


def count_vec_lemma(X_train, X_test):
    vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())
    vectors_train_lemma = vectorizer.fit_transform(X_train)
    vectors_test_lemma = vectorizer.transform(X_test)

    normalizer_train = Normalizer().fit(X=vectors_train_lemma)
    vectors_train_lemma= normalizer_train.transform(vectors_train_lemma)
    vectors_test_lemma = normalizer_train.transform(vectors_test_lemma)

    return vectors_train_lemma, vectors_test_lemma

"""# Measure Accuracies of different classifiers using K-fold Validation

## 1. Logistic Regression

### 1. CountVectorizer
"""

accuracies = []
clf = LogisticRegression(C=1.0, max_iter=1000, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 2. CountVectorizer with stop word"""

accuracies = []
clf = LogisticRegression(C=1.0, max_iter=1000, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_with_sw(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 3. TF-IDF"""

accuracies = []
clf = LogisticRegression(C=40.0, max_iter=1000, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""accuracy vs. hyperparameter C"""

c_vecs = np.logspace(0, 2, 5)
acc = []
for c_vec in c_vecs:
    accuracies = []
    clf = LogisticRegression(C=c_vec, max_iter=1000, random_state=0)
    kf = KFold(n_splits=5, shuffle=True)
    for train_index, test_index in kf.split(X):
        vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
        clf.fit(vectors_train, y[train_index])
        accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

    acc.append(np.mean(accuracies))

plt.xlabel('C')
plt.ylabel('Accuracy')
plt.plot(c_vecs, acc)

"""### 4. CountVectorizer with stem tokenizer"""

accuracies = []
clf = LogisticRegression(C=1.0, max_iter=1000, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_stem(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 5. CountVectorizer with lemma tokenizer"""

accuracies = []
clf = LogisticRegression(C=1.0, max_iter=1000, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_lemma(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""## 2. Multinomial Naïve Bayes

### 1. CountVectorizer
"""

accuracies = []
clf = MultinomialNB()
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 2. CountVectorizer with stop word"""

accuracies = []
clf = MultinomialNB()
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_with_sw(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 3. TF-IDF"""

accuracies = []
clf = MultinomialNB()
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 4. CountVectorizer with stem tokenizer"""

accuracies = []
clf = MultinomialNB()
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_stem(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 5. CountVectorizer with lemma tokenizer"""

accuracies = []
clf = MultinomialNB()
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_lemma(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""## 3. Support Vector Machine

### 1. CountVectorizer
"""

accuracies = []
clf = svm.SVC(kernel='linear', gamma='auto', C=1) 
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 2. CountVectorizer with stop word"""

accuracies = []
clf = svm.SVC(kernel='linear', gamma='auto', C=1) 
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_with_sw(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 3. TF-IDF

Linear
"""

accuracies = []
clf = svm.SVC(kernel='linear', gamma='auto', C=1) 
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""Non-Linear"""

accuracies = []
clf = svm.SVC(gamma='auto', C=1) 
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 4. CountVectorizer with stem tokenizer"""

accuracies = []
clf = svm.SVC(kernel='linear', gamma='auto', C=1) 
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_stem(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 5. CountVectorizer with lemma tokenizer"""

accuracies = []
clf = svm.SVC(kernel='linear', gamma='auto', C=1) 
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_lemma(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""## 4. Random Forest

### 1. CountVectorizer
"""

accuracies = []
clf = RandomForestClassifier(max_depth=2, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 2. CountVectorizer with stop word"""

accuracies = []
clf = RandomForestClassifier(max_depth=2, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_with_sw(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 3. TF-IDF"""

accuracies = []
clf = RandomForestClassifier(max_depth=2, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 4. CountVectorizer with stem tokenizer"""

accuracies = []
clf = RandomForestClassifier(max_depth=2, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_stem(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 5. CountVectorizer with lemma tokenizer"""

accuracies = []
clf = RandomForestClassifier(max_depth=2, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_lemma(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""## 5. Decision Tree

### 1. CountVectorizer
"""

accuracies = []
clf = DecisionTreeClassifier(random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 2. CountVectorizer with stop word"""

accuracies = []
clf = DecisionTreeClassifier(random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_with_sw(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 3. TF-IDF"""

accuracies = []
clf = DecisionTreeClassifier(random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 4. CountVectorizer with stem tokenizer"""

accuracies = []
clf = DecisionTreeClassifier(random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_stem(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 5. CountVectorizer with lemma tokenizer"""

accuracies = []
clf = DecisionTreeClassifier(random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_lemma(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""## 6. Ada Boost

### 1. CountVectorizer
"""

accuracies = []
clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 2. CountVectorizer with stop word"""

accuracies = []
clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_with_sw(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 3. TF-IDF"""

# Commented out IPython magic to ensure Python compatibility.
# find the best hyperparameters
clf = AdaBoostClassifier(random_state=0)
model = make_pipeline(clf)

param_grid = {'adaboostclassifier__n_estimators': [50, 100],
              'adaboostclassifier__learning_rate': [0.1, 0.5, 1]}
grid = GridSearchCV(model, param_grid)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)
vectors_train, vectors_test = tfidf_vectorizer(X_train, X_test)
# %time grid.fit(vectors_train, y_train)
print(grid.best_params_)

accuracies = []
clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 4. CountVectorizer with stem tokenizer"""

accuracies = []
clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_stem(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 5. CountVectorizer with lemma tokenizer"""

accuracies = []
clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=0)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_lemma(X[train_index], X[test_index])
    clf.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""## 7. k-Neighbors

### 1. CountVectorizer
"""

accuracies = []
neigh = KNeighborsClassifier(n_neighbors=3)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vectorizer(X[train_index], X[test_index])
    neigh.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], neigh.predict(vectors_test)))

print(np.mean(accuracies))

"""### 2. CountVectorizer with stop word"""

accuracies = []
neigh = KNeighborsClassifier(n_neighbors=3)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_with_sw(X[train_index], X[test_index])
    neigh.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], neigh.predict(vectors_test)))

print(np.mean(accuracies))

"""### 3. TF-IDF"""

accuracies = []
neigh = KNeighborsClassifier(n_neighbors=3)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
    neigh.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], neigh.predict(vectors_test)))

print(np.mean(accuracies))

"""### 4. CountVectorizer with stem tokenizer"""

accuracies = []
neigh = KNeighborsClassifier(n_neighbors=3)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_stem(X[train_index], X[test_index])
    neigh.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], neigh.predict(vectors_test)))

print(np.mean(accuracies))

"""### 5. CountVectorizer with lemma tokenizer"""

accuracies = []
neigh = KNeighborsClassifier(n_neighbors=3)
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_lemma(X[train_index], X[test_index])
    neigh.fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], neigh.predict(vectors_test)))

print(np.mean(accuracies))

"""## 8. Neural Network

### 1. CountVectorizer
"""

accuracies = []
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vectorizer(X[train_index], X[test_index])
    clf = MLPClassifier(random_state=0, max_iter=300).fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 2. CountVectorizer with stop word"""

accuracies = []
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_with_sw(X[train_index], X[test_index])
    clf = MLPClassifier(random_state=0, max_iter=300).fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 3. TF-IDF"""

accuracies = []
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = tfidf_vectorizer(X[train_index], X[test_index])
    clf = MLPClassifier(random_state=0, max_iter=300).fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 4. CountVectorizer with stem tokenizer"""

accuracies = []
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_stem(X[train_index], X[test_index])
    clf = MLPClassifier(random_state=0, max_iter=300).fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))

"""### 5. CountVectorizer with lemma tokenizer"""

accuracies = []
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    vectors_train, vectors_test = count_vec_lemma(X[train_index], X[test_index])
    clf = MLPClassifier(random_state=0, max_iter=300).fit(vectors_train, y[train_index])
    accuracies.append(metrics.accuracy_score(y[test_index], clf.predict(vectors_test)))

print(np.mean(accuracies))